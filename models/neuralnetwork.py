# -*- coding: utf-8 -*-
"""neuralnetwork.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/15MA7TNzznplxpmXwgKSfit_eXLRZ_n9f

Import libaries
"""

import pandas as pd
import numpy as np
import random
import matplotlib.pyplot as plt

"""Data loading and preprocessing"""

def clean_Q2(q2):
    """
    Clean column Q2 in csv file, converting to all numerical discrete values.
    Handles:
    - Numerical values (e.g., "5", "6.0")
    - Ranges (e.g., "4-6")
    - Textual descriptions (e.g., "around 3")
    - Spelled-out numbers (e.g., "three")
    - Ingredient lists (e.g., "bread, meat, cheese")
    - Multi-line ingredient lists (e.g., "I would expect it to contain:\n* Bread\n* Cheese")
    - #NAME? and other non-numeric values
    """
    # Handle NaN values
    if pd.isna(q2):
        return pd.NA

    # Convert to string if not already
    q2 = str(q2).strip()

    # Handle "I don't know" or similar cases
    if '#NAME?' in q2 or 'don\'t know' in q2.lower() or 'dont know' in q2.lower() or 'no idea' in q2.lower():
        print(f"Skipping Q2 value: {q2}")
        return pd.NA

    # Handle ranges like "4-6" or "5 to 7"
    if '-' in q2 or 'to' in q2 or '~' in q2:
        parts = q2.replace('to', '-').replace('~', '-').split('-')
        if len(parts) == 2 and parts[0].strip().replace('.', '', 1).isdigit() and parts[1].strip().replace('.', '', 1).isdigit():
            low = float(parts[0].strip())
            high = float(parts[1].strip())
            return int((low + high) // 2)  # Return the floored average

    # Handle single numbers (integers or floats)
    if q2.replace('.', '', 1).isdigit():  # Check if it's a number (including floats)
        return int(float(q2))  # Convert to float first, then to integer

    # Handle textual descriptions like "around 5" or "about 3"
    textual_indicators = ["around", "about", "approximately", "~"]
    for indicator in textual_indicators:
        if indicator in q2.lower():
            parts = q2.split()
            for part in parts:
                if part.replace('.', '', 1).isdigit():  # Check if part is a number (including floats)
                    return int(float(part))  # Convert to float first, then to integer

    # Handle cases where the number is spelled out (e.g., "three")
    spelled_out_numbers = {
        'one': 1, 'two': 2, 'three': 3, 'four': 4, 'five': 5,
        'six': 6, 'seven': 7, 'eight': 8, 'nine': 9, 'ten': 10
    }
    for word, num in spelled_out_numbers.items():
        if word in q2.lower():
            return num

    # Handle ingredient lists
    # Check if the text looks like a list of ingredients (may be an issue for those who answered in sentences)
    if ',' in q2 or '\n' in q2 or ' and ' in q2.lower():
        lines = q2.split('\n')
        ingredients = []
        for line in lines:
            line = line.replace('*', '').replace('-', '').replace('â€¢', '').strip()
            if line:
                parts = [part.strip() for part in line.replace(' and ', ',').split(',')]
                ingredients.extend([part for part in parts if part])
        unique_ingredients = set(ingredients)
        return len(unique_ingredients)

    # If no number or ingredients are found, return 'n/a'
    print(f"Could not parse Q2 value: {q2}")
    return pd.NA

def one_hot_encode(response_str, categories):
    result = np.zeros(len(categories))
    if isinstance(response_str, str):  # Check if it's a string (not NaN)
        selections = response_str.split(',')
        for i, category in enumerate(categories):
            if category in selections:
                result[i] = 1
    return result

def categorize_movie_genre(movie_title):
    # Dictionary mapping movie titles to genres
    movie_genres = {
        "Cloudy With A Chance Of Meatballs": "Animation,Comedy",
        "None": "Unknown",
        "Action Movie": "Action",
        "Mamma Mia": "Musical,Comedy",
        "Dragon": "Animation",
        "Rick And Morty": "Animation,Comedy",
        "Home Alone": "Comedy,Family",
        "The Big Lebowski": "Comedy,Crime",
        "Spider-Man": "Superhero,Action",
        "Goodfellas": "Crime,Drama",
        "Dead Silence": "Horror,Thriller",
        "La La Land": "Musical,Drama,Romance",
        "Harry Potter": "Fantasy,Adventure",
        "Transformer": "Action,Sci-Fi",
        "Teenage Mutant Ninja Turtles": "Animation,Action",
        "High School Musical": "Musical,Comedy,Family",
        "Despicable Me": "Animation,Comedy",
        "Toy Story": "Animation,Adventure",
        "The Godfather": "Crime,Drama",
        "Fast And Furious": "Action,Thriller",
        "The Garfield Movie": "Animation,Comedy",
        "Ratatouille": "Animation,Comedy",
        "Five Nights At Freddies": "Horror",
        "Back To The Future": "Sci-Fi,Adventure",
        "Mystic Pizza": "Comedy,Drama,Romance",
        "Iron Man": "Superhero,Action",
        "Life Of Pi": "Adventure,Drama",
        "Avengers": "Superhero,Action",
        "King Kong": "Action,Adventure,Drama",
        "Whiplash": "Drama,Music",
        "Inside Out": "Animation,Drama",
        "Superbad": "Comedy",
        "Stranger Things": "Sci-Fi,Horror,Drama",
        "Air Bud": "Family,Comedy",
        "Deadpool": "Action,Comedy",
        "A Quiet Place": "Horror,Thriller",
        "Finding Nemo": "Animation,Adventure",
        "My Cousin Vinny": "Comedy,Crime",
        "Eat Pray Love": "Drama,Romance",
        "Pulp Fiction": "Crime,Thriller",
        "Star Wars": "Sci-Fi,Adventure",
        "Batman": "Superhero,Action",
        "Shrek": "Animation,Comedy",
        "Scooby Doo": "Animation,Adventure",
        "Princess Diaries": "Comedy,Family,Romance",
        "Wall-E": "Animation,Sci-Fi",
        "The Hangover": "Comedy",
        "Breaking Bad": "Crime,Drama",
        "Interstellar": "Sci-Fi,Adventure,Drama",
        "Rush Hour": "Action,Comedy",
        "The Truman Show": "Drama,Sci-Fi",
        "Futurama": "Animation,Comedy,Sci-Fi",
        "Godfather": "Crime,Drama",
        "The Dictator": "Comedy,Political",
        "Borat": "Comedy,Political",
        "Mission: Impossible": "Action,Thriller",
        "Avengers: Endgame": "Superhero,Action",
        "Titanic": "Drama,Romance",
        "Dangal": "Drama,Sports",
        "Kung Fu Panda": "Animation,Action",
        "The Mummy": "Action,Adventure",
        "The Invisible Guest": "Thriller,Mystery",
        "Squid Game": "Drama,Thriller",
        "Parasite": "Thriller,Drama",
        "Blade Runner": "Sci-Fi,Thriller",
        "Spider-Man: Into The Spider-Verse": "Animation,Action",
        "Everything Everywhere All At Once": "Sci-Fi,Action,Adventure",
        "Barbie": "Comedy,Family",
        "Jurassic Park": "Adventure,Sci-Fi",
        "Ponyo": "Animation,Fantasy",
        "My Neighbor Totoro": "Animation,Family",
        "Kill Bill": "Action,Thriller",
        "Jiro Dreams Of Sushi": "Documentary",
        "Naruto": "Animation,Action",
        "Frozen": "Animation,Adventure,Family",
        "Shawshank Redemption": "Drama",
        "Mad Max": "Action,Sci-Fi",
        "The Lion King": "Animation,Adventure,Drama",
        "Your Name": "Animation,Romance,Fantasy",
        "Memoirs Of A Geisha": "Drama,Romance",
        "Godzilla": "Action,Sci-Fi",
        "Shazam": "Superhero,Action",
        "The Grinch": "Animation,Comedy,Family",
        "Zootopia": "Animation,Adventure,Comedy",
        "The Godfather Part II": "Crime,Drama",
        "The Social Network": "Drama",
        "The Big Sick": "Comedy,Drama,Romance",
        "Die Hard": "Action,Thriller",
        "Taxi Driver": "Crime,Drama,Thriller",
        "Fast & Furious": "Action,Thriller",
        "The Karate Kid": "Drama,Family",
        "John Wick": "Action,Thriller",
        "Bladerunner": "Sci-Fi,Thriller",
        "Parasite": "Thriller,Drama",
        "Gone Girl": "Thriller,Drama",
        "Inception": "Sci-Fi,Thriller",
        "The Breakfast Club": "Comedy,Drama",
        "The Lego Movie": "Animation,Adventure,Comedy",
        "Spider-Man: Far From Home": "Superhero,Action",
        "Space Jam": "Animation,Comedy",
        "Spongebob": "Animation,Comedy",
        "Toy Story 4": "Animation,Adventure",
        "Green Book": "Drama",
        "Madagascar": "Animation,Adventure,Comedy",
        "The Mario Movie": "Animation,Adventure",
        "The Big Lebowski": "Comedy,Crime",
        "The Road To Fallujah": "Documentary",
        "Kingdom Of Heaven": "Action,Adventure,Drama",
        "The Dictator": "Comedy,Political",
        "Rush Hour": "Action,Comedy",
        "Breaking Bad": "Crime,Drama",
        "The Boys": "Superhero,Action",
        "Wicked": "Musical,Fantasy",
        "Eternal Sunshine Of The Spotless Mind": "Drama,Romance,Sci-Fi",
        "The Grinch": "Animation,Comedy,Family",
        "Ratatouille": "Animation,Comedy",
        "Star Wars: The Last Jedi": "Sci-Fi,Action",
        "The Dictator": "Comedy",
        "The Lego Movie 2": "Animation,Comedy,Adventure",
        "Dune": "Sci-Fi,Adventure"
    }

    # Handle None or empty strings
    if movie_title is None or str(movie_title).lower() in ['none', 'nan'] or not str(movie_title).strip():
        return "no_movie"

    # Normalize the title for comparison
    normalized_title = str(movie_title)

    # Check for direct matches
    if normalized_title in movie_genres:
        return movie_genres[normalized_title]

    # Check for partial matches
    for title, genre in movie_genres.items():
        if title in normalized_title or normalized_title in title:
            return genre

    # If no match is found
    return "other"


def evaluate_model(model, X_test, y_test):
    """
    Evaluate the model's accuracy without using the torch library.
    """
    # Get predictions from the model
    predicted = model.predict(X_test)

    # Calculate the number of correct predictions
    correct = (predicted == y_test).sum()

    # Calculate accuracy
    accuracy = correct / len(y_test)

    return accuracy

df = pd.read_csv('../data/final_processed.csv')  # already processed


# list the columns
print(df.columns)

"""### Model Building: Forward Pass"""

def relu(x):
    """ReLU activation function."""
    return np.maximum(0, x)

def relu_derivative(x):
    """Derivative of ReLU activation function."""
    return (x > 0).astype(float)

def softmax(z):
    """
    Compute the softmax of vector z, or row-wise for a matrix z.
    For numerical stability, subtract the maximum logit value from each
    row prior to exponentiation (see above).

    Parameters:
        `z` - a numpy array of shape (K,) or (N, K)

    Returns: a numpy array with the same shape as `z`, with the softmax
        activation applied to each row of `z`
    """
    exp_z= np.exp(z - np.max(z, axis=1, keepdims=True))
    return exp_z/ np.sum(exp_z, axis=1, keepdims=True)

class FoodNeuralNetwork(object):
    def __init__(self, num_features=128*20, num_hidden=100, num_classes=128):
        """
        Initialize the weights and biases of this two-layer MLP.
        """
        # information about the model architecture
        self.num_features = num_features
        self.num_hidden = num_hidden
        self.num_classes = num_classes

        # weights and biases for the first layer of the MLP
        self.W1 = np.zeros([num_hidden, num_features])
        self.b1 = np.zeros([num_hidden])

        # weights and biases for the second layer of the MLP
        self.W2 = np.zeros([num_classes, num_hidden])
        self.b2 = np.zeros([num_classes])

        # initialize the weights and biases
        self.initializeParams()

        # set all values of intermediate variables (to be used in the
        # forward/backward passes) to None
        self.cleanup()

    def initializeParams(self):
        """
        Initialize the weights and biases of this two-layer MLP to be random.
        This random initialization is necessary to break the symmetry in the
        gradient descent update for our hidden weights and biases. If all our
        weights were initialized to the same value, then their gradients will
        all be the same!
        """
        self.W1 = np.random.normal(0, 2/self.num_features, self.W1.shape)
        self.b1 = np.random.normal(0, 2/self.num_features, self.b1.shape)
        self.W2 = np.random.normal(0, 2/self.num_hidden, self.W2.shape)
        self.b2 = np.random.normal(0, 2/self.num_hidden, self.b2.shape)

    def forward(self, X):
        """
        Compute the forward pass to produce prediction for inputs.

        Parameters:
            `X` - A numpy array of shape (N, self.num_features)

        Returns: A numpy array of predictions of shape (N, self.num_classes)
        """
        return do_forward_pass(self, X) # To be implemented below

    def backward(self, ts):
        """
        Compute the backward pass, given the ground-truth, one-hot targets.

        You may assume that the `forward()` method has been called for the
        corresponding input `X`, so that the quantities computed in the
        `forward()` method is accessible.

        Parameters:
            `ts` - A numpy array of shape (N, self.num_classes)
        """
        return do_backward_pass(self, ts) # To be implemented below

    def loss(self, ts):
        """
        Compute the average cross-entropy loss, given the ground-truth, one-hot targets.

        You may assume that the `forward()` method has been called for the
        corresponding input `X`, so that the quantities computed in the
        `forward()` method is accessible.

        Parameters:
            `ts` - A numpy array of shape (N, self.num_classes)
        """
        return np.sum(-ts * np.log(self.y)) / ts.shape[0]

    def update(self, alpha):
        """
        Compute the gradient descent update for the parameters of this model.

        Parameters:
            `alpha` - A number representing the learning rate
        """
        self.W1 = self.W1 - alpha * self.W1_bar
        self.b1 = self.b1 - alpha * self.b1_bar
        self.W2 = self.W2 - alpha * self.W2_bar
        self.b2 = self.b2 - alpha * self.b2_bar

    def cleanup(self):
        """
        Erase the values of the variables that we use in our computation.
        """
        # To be filled in during the forward pass
        self.N = None # Number of data points in the batch
        self.X = None # The input matrix
        self.m = None # Pre-activation value of the hidden state, should have shape
        self.h = None # Post-RELU value of the hidden state
        self.z = None # The logit scores (pre-activation output values)
        self.y = None # Class probabilities (post-activation)
        # To be filled in during the backward pass
        self.z_bar = None # The error signal for self.z2
        self.W2_bar = None # The error signal for self.W2
        self.b2_bar = None # The error signal for self.b2
        self.h_bar = None  # The error signal for self.h
        self.m_bar = None # The error signal for self.z1
        self.W1_bar = None # The error signal for self.W1
        self.b1_bar = None # The error signal for self.b1


def do_forward_pass(model, X):
    """
    Compute the forward pass to produce prediction for inputs.

    This function also keeps some of the intermediate values in
    the neural network computation, to make computing gradients easier.

    For the ReLU activation, you may find the function `np.maximum` helpful

    Parameters:
        `model` - An instance of the class MLPModel
        `X` - A numpy array of shape (N, model.num_features)

    Returns: A numpy array of predictions of shape (N, model.num_classes)
    """
    model.N = X.shape[0]
    model.X = X
    # X = (8, 10)
    # W1 = (20, 10)
    model.m = X.dot(model.W1.T) + model.b1 # TODO - the hidden state value (pre-activation)
    model.h = np.maximum(0, model.m) # TODO - the hidden state value (post ReLU activation)
    model.z = model.h.dot(model.W2.T) + model.b2  # TODO - the logit scores (pre-activation)
    model.y = softmax(model.z) # TODO - the class probabilities (post-activation)
    return model.y

def do_backward_pass(model, ts):
    """
    Compute the backward pass, given the ground-truth, one-hot targets.

    You may assume that `model.forward()` has been called for the
    corresponding input `X`, so that the quantities computed in the
    `forward()` method is accessible.

    The member variables you store here will be used in the `update()`
    method. Check that the shapes match what you wrote in Part 2.

    Parameters:
        `model` - An instance of the class MLPModel
        `ts` - A numpy array of shape (N, model.num_classes)
    """
    model.z_bar = (model.y - ts) / model.N
    model.W2_bar = model.z_bar.T.dot(model.h)
    model.b2_bar = np.sum(model.z_bar, axis=0)
    model.h_bar = model.z_bar.dot(model.W2)
    model.m_bar = model.h_bar * (model.m > 0)
    model.W1_bar = model.m_bar.T.dot(model.X)
    model.b1_bar = np.sum(model.m_bar, axis=0)

def train_sgd(model, X_train, t_train,
              alpha=0.1, n_epochs=0, batch_size=100,
              X_valid=None, t_valid=None,
              w_init=None, plot=True):
    '''
    Given `model` - an instance of MLPModel
          `X_train` - the data matrix to use for training
          `t_train` - the target vector to use for training
          `alpha` - the learning rate.
                    From our experiments, it appears that a larger learning rate
                    is appropriate for this task.
          `n_epochs` - the number of **epochs** of gradient descent to run
          `batch_size` - the size of each mini batch
          `X_valid` - the data matrix to use for validation (optional)
          `t_valid` - the target vector to use for validation (optional)
          `w_init` - the initial `w` vector (if `None`, use a vector of all zeros)
          `plot` - whether to track statistics and plot the training curve

    Solves for model weights via stochastic gradient descent,
    using the provided batch_size.

    Return weights after `niter` iterations.
    '''
    def make_onehot(indicies, total=128):
        I = np.eye(total)
        return I[indicies]
    # as before, initialize all the weights to zeros
    w = np.zeros(X_train.shape[1])

    train_loss = [] # for the current minibatch, tracked once per iteration
    valid_loss = [] # for the entire validation data set, tracked once per epoch

    # track the number of iterations
    niter = 0

    # we will use these indices to help shuffle X_train
    N = X_train.shape[0] # number of training data points
    indices = list(range(N))

    for e in range(n_epochs):
        random.shuffle(indices) # for creating new minibatches

        for i in range(0, N, batch_size):
            if (i + batch_size) > N:
                # At the very end of an epoch, if there are not enough
                # data points to form an entire batch, then skip this batch
                continue

            indices_in_batch = indices[i: i+batch_size]
            X_minibatch = X_train[indices_in_batch, :]
            t_minibatch = make_onehot(t_train[indices_in_batch], model.num_classes)

            # gradient descent iteration
            model.cleanup()
            model.forward(X_minibatch)
            model.backward(t_minibatch)
            model.update(alpha)

            if plot:
                # Record the current training loss values
                train_loss.append(model.loss(t_minibatch))
            niter += 1

        # compute validation data metrics, if provided, once per epoch
        if plot and (X_valid is not None) and (t_valid is not None):
            model.cleanup()
            model.forward(X_valid)
            valid_loss.append((niter, model.loss(make_onehot(t_valid, model.num_classes))))

    if plot:
        plt.title("SGD Training Curve Showing Loss at each Iteration")
        plt.plot(train_loss, label="Training Loss")
        if (X_valid is not None) and (t_valid is not None): # compute validation data metrics, if provided
            plt.plot([iter for (iter, loss) in valid_loss],
                     [loss for (iter, loss) in valid_loss],
                     label="Validation Loss")
        plt.xlabel("Iterations")
        plt.ylabel("Loss")
        plt.legend()
        plt.show()
        print("Final Training Loss:", train_loss[-1])
        if (X_valid is not None) and (t_valid is not None):
            print("Final Validation Loss:", valid_loss[-1])

def train_test_split(X, y, test_size=0.2, random_state=None):
    """
    Split data into training and testing sets.

    Parameters:
        X (numpy.ndarray): Feature matrix.
        y (numpy.ndarray): Labels array.
        test_size (float): Proportion of the dataset to include in the test split (default 0.2).
        random_state (int): Seed for reproducibility (default None).

    Returns:
        X_train, X_test, y_train, y_test: Split datasets.
    """
    if random_state is not None:
        np.random.seed(random_state)  # Set random seed for reproducibility

    # Shuffle the data
    indices = np.arange(X.shape[0])
    np.random.shuffle(indices)

    # Apply the shuffled indices to X and y
    X = X[indices]
    y = y[indices]

    # Compute the split index
    split_idx = int(X.shape[0] * (1 - test_size))

    # Split the data
    X_train, X_test = X[:split_idx], X[split_idx:]
    y_train, y_test = y[:split_idx], y[split_idx:]

    X_train = X_train.astype(int)
    X_test = X_test.astype(int)

    return X_train, X_test, y_train, y_test


feature_columns = [
    "Q1", "Q2", "Q4",

    # Q3 (settings)
    "Q3_Week day lunch",
    "Q3_Week day dinner",
    "Q3_Weekend lunch",
    "Q3_Weekend dinner",
    "Q3_At a party",
    "Q3_Late night snack",

    # Q6 (drinks)
    "Q6_soda", "Q6_other", "Q6_tea", "Q6_alcohol", "Q6_water",
    "Q6_soup", "Q6_juice", "Q6_milk", "Q6_unknown", "Q6_smoothie",
    "Q6_asian alcohol", "Q6_asian pop", "Q6_milkshake",

    # Q7 (people)
    "Q7_Parents", "Q7_Siblings", "Q7_Friends", "Q7_Teachers", "Q7_Strangers",

    # Q8 (hot sauce levels)
    "Q8_None",
    "Q8_A little (mild)",
    "Q8_A moderate amount (medium)",
    "Q8_A lot (hot)",
    "Q8_I will have some of this food item with my hot sauce",

    # Q5 (movie genres)
    "genre_Thriller", "genre_Fantasy", "genre_Mystery", "genre_Animation",
    "genre_Comedy", "genre_Sci-Fi", "genre_Horror", "genre_Documentary",
    "genre_Sports", "genre_Action", "genre_Romance", "genre_other",
    "genre_Superhero", "genre_Music", "genre_Family", "genre_Crime",
    "genre_Musical", "genre_Drama", "genre_Adventure", "genre_Political"
]

# summarize df
df = df[feature_columns + ['Label']]

if "id" in df.columns:
    df = df.drop(columns=["id"])

features = df[feature_columns].fillna(0).values
target = df['Label'].map({'Pizza': 0, 'Shawarma': 1, 'Sushi': 2}).values

# create a small subset of the data for testing
X = features[:1000]
y = target[:1000]
model = FoodNeuralNetwork(num_features=X.shape[1], num_hidden=100, num_classes=3)
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)
train_sgd(model, X_train=X_train, t_train=y_train, alpha=0.2, batch_size=100, n_epochs=500)

X = df[feature_columns].fillna(0).values
y = df['Label'].map({'Pizza': 0, 'Shawarma': 1, 'Sushi': 2}).values  # Convert labels to integers

X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.4, random_state=42)

model = FoodNeuralNetwork(num_features=X_train.shape[1], num_hidden=100, num_classes=3)

train_sgd(model, X_train, y_train, alpha=0.1, n_epochs=200, batch_size=128, X_valid=X_test, t_valid=y_test)

from sklearn.model_selection import ParameterGrid

param_grid = {
    'alpha': [0.001, 0.01, 0.1],
    'n_epochs': [50, 100, 200],
    'batch_size': [32, 64, 128],
}

for params in ParameterGrid(param_grid):
    train_sgd(
        model, X_train, y_train,
        alpha=params['alpha'],
        n_epochs=params['n_epochs'],
        batch_size=params['batch_size'],
        X_valid=X_test,
        t_valid=y_test
    )