# -*- coding: utf-8 -*-
"""naive_bayes_model.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1bnpxsx3qTP3_7wmNVnJKvmxWyLynASHK

Mount drive and get csv file
"""

data_string = "data/worded_data.csv"

import csv
data = csv.reader(open(data_string))

import numpy as np

vocab = None # TODO

vocab_set = set()
data = csv.reader(open(data_string))
for i, line in enumerate(data):
  if i == 0:
    continue
  review = [word for sublist in line[1:9] for word in sublist.split(",")]
  words = set(review)
  for w in words:
    vocab_set.add(w)

vocab = list(vocab_set)

print(vocab_set)

print("Vocabulary Size: ", len(vocab)) # Please include the output of this statement in your submission

"""Bag of words"""



def make_bow(data, vocab):
    """
    Produce the bag-of-word representation of the data, along with a vector
    of labels. You *may* use loops to iterate over `data`. However, your code
    should not take more than O(len(data) * len(vocab)) to run.

    Parameters:
        `data`: a list of `(review, label)` pairs, like those produced from
                `list(csv.reader(open("trainvalid.csv")))`
        `vocab`: a list consisting of all unique words in the vocabulary

    Returns:
        `X`: A data matrix of bag-of-word features. This data matrix should be
             a numpy array with shape [len(data), len(vocab)].
             Moreover, `X[i,j] == 1` if the review in `data[i]` contains the
             word `vocab[j]`, and `X[i,j] == 0` otherwise.
        `t`: A numpy array of shape [len(data)], with `t[i] == 1` if
             `data[i]` is a positive review, and `t[i] == 0` otherwise.
    """
    X = np.zeros([len(data), len(vocab)])
    t = np.zeros([len(data)])
    vocab_dict = {word: idx for idx, word in enumerate(vocab)}
    print(vocab_dict)

    for i, row in enumerate(data):
        if row[9] == 'Pizza':
          t[i] = 0
        elif row[9] == 'Shawarma':
          t[i] = 1
        elif row[9] == "Sushi":
          t[i] = 2
        review = [word for sublist in row[1:9] for word in sublist.split(",")]

        print(review)
        words = set(review)
        for word in words:
            if word in vocab_dict:
                X[i, vocab_dict[word]] = 1
    return X, t




# Separate data into training/validation, then call `make_bow` to produce
# the bag-of-word features
import random
data = csv.reader(open(data_string))
random.seed(42)
data_list = list(data)
# print(data)
random.shuffle(data_list)
# print(data_list)

X_train, t_train = make_bow(data_list[:820], vocab)
X_valid, t_valid = make_bow(data_list[820:], vocab)
print(X_train)

"""MAP - calculate Pi and Theta

"""

def naive_bayes_map(X, t):
    r"""
    Compute the parameters $pi$ and $theta_{jc}$ that maximizes the posterior
    of the provided data (X, t). We will use the beta distribution with
    $a=2$ and $b=2$ for all of our parameters.

    **Your solution should be vectorized, and contain no loops**

    Parameters:
        `X` - a matrix of bag-of-word features of shape [N, V],
              where N is the number of data points and V is the vocabulary size.
              X[i,j] should be either 0 or 1. Produced by the make_bow() function.
        `t` - a vector of class labels of shape [N], with t[i] being either 0 or 1.
              Produced by the make_bow() function.

    Returns:
        `pi` - a scalar; the MAP estimate of the parameter $\pi = p(c = 1)$
        `theta` - a matrix of shape [V, 2], where `theta[j, c]` corresponds to
                  the MAP estimate of the parameter $\theta_{jc} = p(x_j = 1 | c)$
    """
    N, vocab_size = X.shape[0], X.shape[1]
    pi2 = (np.sum(t == 2) + 1) / (N + 2)
    pi1 = (np.sum(t == 1) + 1) / (N + 2)
    pi0 = (np.sum(t == 0) + 1) / (N + 2)
    theta = np.zeros([vocab_size, 3]) # TODO

    # these matrices may be useful (but what do they represent?)
    X_pizza = X[t == 0]
    X_shawarma = X[t == 1]
    X_sushi = X[t == 2]

    theta[:, 0] = (np.sum(X_pizza, axis=0) + 2 - 1) / (X_pizza.shape[0] + 2 + 2 -2) # TODO: you may uncomment this line if you'd like
    theta[:, 1] = (np.sum(X_shawarma, axis=0) + 2 - 1) / (X_shawarma.shape[0] + 2 + 2 -2)
    theta[:, 2] = (np.sum(X_sushi, axis=0) + 2 - 1) / (X_sushi.shape[0] + 2 + 2 -2)

    return pi0, pi1, pi2, theta

pi0, pi1, pi2, theta_map = naive_bayes_map(X_train, t_train)

print(theta_map.shape) # should be (549, 2)
print(pi0)
print(pi1)
print(pi2)

def make_prediction(X, pi0, pi1, pi2, theta):
    """
    Make predictions using Naive Bayes with 3 classes

    Parameters:
        X - bag of words features [N, V]
        pi0, pi1, pi2 - prior probabilities for each class
        theta - probability matrix [V, 3] where theta[j,c] is P(word j|class c)

    Returns:
        y - predicted classes [N]
    """
    N = X.shape[0]  # Number of samples
    y = np.zeros(N)  # Initialize predictions vector

    # Calculate log prior probabilities
    log_p0 = np.log(pi0)
    log_p1 = np.log(pi1)
    log_p2 = np.log(pi2)

    # Calculate log conditional probabilities for each class
    # Class 0 (Pizza)
    log_theta_0 = np.log(theta[:, 0])
    log_one_minus_theta_0 = np.log(1 - theta[:, 0])
    log_contrib_0 = np.dot(X, log_theta_0) + np.dot((1 - X), log_one_minus_theta_0)

    # Class 1 (Shawarma)
    log_theta_1 = np.log(theta[:, 1])
    log_one_minus_theta_1 = np.log(1 - theta[:, 1])
    log_contrib_1 = np.dot(X, log_theta_1) + np.dot((1 - X), log_one_minus_theta_1)

    # Class 2 (Sushi)
    log_theta_2 = np.log(theta[:, 2])
    log_one_minus_theta_2 = np.log(1 - theta[:, 2])
    log_contrib_2 = np.dot(X, log_theta_2) + np.dot((1 - X), log_one_minus_theta_2)

    # Calculate total log probabilities for each class
    log_p0_total = log_p0 + log_contrib_0
    log_p1_total = log_p1 + log_contrib_1
    log_p2_total = log_p2 + log_contrib_2

    # Stack log probabilities into a matrix [N, 3]
    log_probs = np.column_stack([log_p0_total, log_p1_total, log_p2_total])

    # For each sample, predict the class with highest probability
    y = np.argmax(log_probs, axis=1)

    return y

def accuracy(y, t):
    return np.mean(y == t)


y_map_train = make_prediction(X_train, pi0, pi1, pi2, theta_map)

y_map_valid = make_prediction(X_valid, pi0, pi1, pi2, theta_map)
print(y_map_valid)
print("MAP Train Acc:", accuracy(y_map_train, t_train))
print("MAP Valid Acc:", accuracy(y_map_valid, t_valid))