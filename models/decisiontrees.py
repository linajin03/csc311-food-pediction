# -*- coding: utf-8 -*-
"""DecisionTrees.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/18W5XFpNvYyK72V1yRXXEYqcOD_AOI064
"""

import matplotlib.pyplot as plt # For plotting
import numpy as np              # Linear algebra library
import pandas as pd
import csv

from google.colab import drive
drive.mount('/content/drive', force_remount=True)
path_to_data = "/content/drive/MyDrive/0uoft/CleanedData.csv"

# read each of the csv files as a *pandas data frame*
data = pd.read_csv(path_to_data)

# display one the dataframes in the notebook
data

data.describe()

data.boxplot(column = "Q1", by='Label')

data.boxplot(column = "Q2", by='Label')

data.boxplot(column = "Q4", by='Label')

#pd.crosstab(data["Q3"], data["Label"])
data

data_fets = np.stack([
    # Q1: complexity, Q2: # of ingredients
    data["Q1"],
    data["Q2"],
    # Create seperate indicator variables for selections in Q3
    [1 if "Week day lunch" in item else 0 for item in data["Q3"]],
    [1 if "Week day dinner" in item else 0 for item in data["Q3"]],
    [1 if "Weekend lunch" in item else 0 for item in data["Q3"]],
    [1 if "Weekend dinner" in item else 0 for item in data["Q3"]],
    [1 if "Late night snack" in item else 0 for item in data["Q3"]],
    # Q4: Price
    data["Q4"],
    # Q5: Movie
    #data["Q5"],
    # Q6: Drink
    #data["Q6"],
    # Create seperate indicator variables for selections in Q
    [1 if (not isinstance(item, float) and "Parents" in item) else 0 for item in data["Q7"]],
    [1 if (not isinstance(item, float) and "Siblings" in item) else 0 for item in data["Q7"]],
    [1 if (not isinstance(item, float) and "Friends" in item) else 0 for item in data["Q7"]],
    [1 if (not isinstance(item, float) and "Teachers" in item) else 0 for item in data["Q7"]],
    [1 if (not isinstance(item, float) and "Strangers" in item) else 0 for item in data["Q7"]],
    #Q8: Hot Sauce
    data["Q8"],
    data["Label"],
], axis=1)

print(data_fets[2]) # Should be (8000, 14)

import graphviz
from sklearn import tree as treeViz

feature_names = ["Complexity", "Ingredients", "wdl", "wdd", "wel", "wed", "lns", "Price", "Parent", "Sibling", "Friends", "Teachers", "Strangers", "Hot Sauce"]

def visualize_tree(model, max_depth=5):
    """
    Generate and return an image representing an Sklearn decision tree.

    Each node in the visualization represents a node in the decision tree.
    In addition, visualization for each node contains:
        - The feature that is split on
        - The entropy (of the outputs `t`) at the node
        - The number of training samples at the node
        - The number of training samples with true/false values
        - The majority class (heart disease or not)
    The colour of the node also shows the majority class and purity

    See here: https://scikit-learn.org/stable/modules/generated/sklearn.tree.export_graphviz.html

    Parameters:
        `model` - An Sklearn decision tree model
        `max_depth` - Max depth of decision tree to be rendered in the notebook.
         This is useful since the tree can get very large if the max_depth is
         set too high and thus making the resulting figure difficult to interpret.
    """
    dot_data = treeViz.export_graphviz(model,
                                       feature_names=feature_names,
                                       max_depth=max_depth,
                                       class_names=["Pizza", "Shwarma", "Sushi"],
                                       filled=True,
                                       rounded=True)
    return display(graphviz.Source(dot_data))



# prompt: seperate "data_fets" into training, validation and test sets, and then fit a decision tree to determine labels from "data_fets"

from sklearn.model_selection import train_test_split
from sklearn.tree import DecisionTreeClassifier

# Assuming data_fets is already defined as in the provided code

# Separate features (X) and labels (y)
X = data_fets[:, :-1]  # All columns except the last one (Label)
y = data_fets[:, -1]   # The last column (Label)

# Split data into training, validation, and test sets
X_train, X_temp, y_train, y_temp = train_test_split(X, y, test_size=0.3, random_state=42)  # 70% train, 30% temp
X_val, X_test, y_val, y_test = train_test_split(X_temp, y_temp, test_size=0.5, random_state=42) # Split temp into 50% validation, 50% test


# Initialize and train a Decision Tree Classifier
clf = DecisionTreeClassifier(criterion="entropy", max_depth=10, min_samples_split=20)
clf.fit(X_train, y_train)





# You can now use clf.predict(X_val) and clf.predict(X_test) to get predictions on the validation and test sets respectively
# Evaluate the model using appropriate metrics (accuracy, precision, recall, F1-score, etc.)
# Example:
from sklearn.metrics import accuracy_score

y_pred_train = clf.predict(X_train)
accuracy_train = accuracy_score(y_train, y_pred_train)
print(f"Training Accuracy: {accuracy_train}")

y_pred_val = clf.predict(X_val)
accuracy_val = accuracy_score(y_val, y_pred_val)
print(f"Validation Accuracy: {accuracy_val}")

y_pred_test = clf.predict(X_test)
accuracy_test = accuracy_score(y_test, y_pred_test)
print(f"Test Accuracy: {accuracy_test}")

# prompt: tune decision tree hyperparameters to maximise test accuracy

from sklearn.model_selection import GridSearchCV

# Define the parameter grid to search
param_grid = {
    'criterion': ['gini', 'entropy'],
    'max_depth': [None, 10, 20, 30, 40, 50],
    'min_samples_split': [2, 5, 10, 20, 30, 40],
    'min_samples_leaf': [1, 2, 4, 8, 16]
}

# Create a GridSearchCV object
grid_search = GridSearchCV(estimator=DecisionTreeClassifier(), param_grid=param_grid, cv=5, scoring='accuracy')

# Fit the grid search to the training data
grid_search.fit(X_train, y_train)

# Get the best hyperparameters and the best model
best_params = grid_search.best_params_
best_model = grid_search.best_estimator_

print(f"Best Hyperparameters: {best_params}")

# Evaluate the best model on the test set
y_pred_test = best_model.predict(X_test)
accuracy_test = accuracy_score(y_test, y_pred_test)
print(f"Test Accuracy with Best Hyperparameters: {accuracy_test}")

visualize_tree(clf)

